{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb332d2",
   "metadata": {},
   "source": [
    "## Web Scraping with LLaMA 3: Turn Any Website into Structured JSON (2025 Guide)\n",
    "\n",
    "With LLaMA 3, I can easily turn messy HTML into neat, structured JSON. Itâ€™s a smarter and more reliable way to scrape data, making the whole process smoother and less prone to errors. Letâ€™s dive into how it works!\n",
    "\n",
    "What is LLaMA 3?\n",
    "LLaMA 3, which stands for Large Language Model Meta AI, is Metaâ€™s third iteration of an open-weight language model, released in April 2024. It is a large language model that supports various tasks, from text generation to natural language understanding. LLaMA 3 models are available in different sizes, ranging from 8 billion parameters (8B) to 405 billion parameters (405B). These models are designed to run efficiently on various hardware setups, making them accessible for many developers and businesses.\n",
    "\n",
    "Unlike traditional scraping methods, which rely on predefined selectors, LLaMA 3 understands the content contextually. This means it can intelligently extract information from web pages, even if the layout changes or the website has complex, dynamically loaded content.\n",
    "\n",
    "Why Use LLaMA 3 for Web Scraping?\n",
    "Web scraping has become more complicated over time due to several factors:\n",
    "\n",
    "Dynamic Content: Many websites now load content dynamically using JavaScript. This makes traditional scraping methods that rely on static HTML selectors ineffective.\n",
    "Website Layout Changes: Web pages frequently update their designs, breaking scraping scripts that depend on specific element locations.\n",
    "Anti-Bot Protections: Many websites have implemented anti-bot measures, such as CAPTCHA challenges, IP blocking, and JavaScript-based protections, making it harder to scrape data without getting blocked.\n",
    "Traditional scraping methods, like using XPath or CSS selectors, are fragile because they break when the website layout changes. However, LLaMA 3 offers a new way of handling these challenges. It can parse the content contextually, making it less likely to break due to minor changes in the website layout.\n",
    "\n",
    "Some of the benefits of using LLaMA 3 for web scraping include:\n",
    "\n",
    "Flexibility: LLaMA 3 can adapt to different website structures, making it perfect for scraping dynamic and frequently changing websites.\n",
    "Efficiency: By converting raw HTML into clean, structured data in JSON format, LLaMA 3 makes it easier to store and process scraped information.\n",
    "Reliability: The modelâ€™s contextual understanding of content ensures that it extracts only relevant data, reducing the risk of errors.\n",
    "ðŸ”— Boost Web Scraping with Bright Dataâ€™s MCP\n",
    "For more reliable and up-to-date results, consider integrating Bright Dataâ€™s Model Context Protocol (MCP) with your LLaMA 3 setup. MCP provides real-time web access, bypasses geo-restrictions and bot protections, and ensures your model processes the freshest data available. Itâ€™s especially useful when scraping dynamic or protected sites â€” making your pipeline more robust and accurate without extra complexity.\n",
    "\n",
    "Try it here.\n",
    "\n",
    "\n",
    "Setting Up Your Environment for LLaMA 3\n",
    "Before diving into the scraping process, there are a few prerequisites that you need to have in place:\n",
    "\n",
    "Python 3: Make sure Python 3 is installed on your system. This guide assumes you have basic Python knowledge.\n",
    "Operating System Compatibility: LLaMA 3 works on macOS (Big Sur or later), Linux, and Windows (10 or later).\n",
    "Hardware Resources: Depending on your chosen model size, youâ€™ll need sufficient RAM and disk space. For example, LLaMA 3.1 with 8 billion parameters requires about 6â€“8 GB of RAM and 4.9 GB of disk space.\n",
    "Once your environment is ready, youâ€™ll need to install Ollama, a tool for downloading, setting up, and running LLaMA models locally.\n",
    "\n",
    "Installing Ollama\n",
    "Ollama simplifies the process of downloading and setting up LLaMA 3 models. To get started:\n",
    "\n",
    "Visit the official Ollama website and download the application compatible with your operating system.\n",
    "Follow the installation instructions provided on the website.\n",
    "After installing Ollama, youâ€™ll need to select the right model based on your hardware and use case. For most users, the llama3.1:8b model is ideal because it offers a good balance between performance and resource requirements.\n",
    "\n",
    "Running LLaMA 3 Locally\n",
    "Once youâ€™ve installed Ollama, you can begin using LLaMA 3 by following these steps:\n",
    "\n",
    "Download the Model: Open your terminal and run the following command to download the LLaMA 3.1 model:\n",
    "ollama run llama3.1:8b\n",
    "This command will download the model and start an interactive prompt where you can test the model by sending queries like:\n",
    "\n",
    ">>> who are you?\n",
    "\n",
    "I am LLaMA, an AI assistant developed by Meta AIâ€¦\n",
    "\n",
    "Start the Ollama Server: To run the LLaMA model as a local server, use the following command:\n",
    "ollama serve\n",
    "This starts the server at http://127.0.0.1:11434/, which you can access from your browser to verify that the server is running.\n",
    "\n",
    "Building a Web Scraper Using LLaMA 3\n",
    "Now that LLaMA 3 is set up and running, letâ€™s walk through the process of building a simple web scraper to extract product information from an e-commerce website like Amazon.\n",
    "\n",
    "The scraper will follow a multi-stage workflow:\n",
    "\n",
    "Browser Automation: Use Selenium to load the page and render dynamic content.\n",
    "HTML Extraction: Identify the product details container on the webpage.\n",
    "Markdown Conversion: Convert the HTML content to Markdown for better processing by LLaMA.\n",
    "Data Extraction with LLaMA: Use LLaMA to extract structured data and convert it into JSON format.\n",
    "Output Handling: Save the extracted data to a JSON file for further analysis.\n",
    "Step 1: Install Required Libraries\n",
    "To get started, install the necessary Python libraries:\n",
    "\n",
    "pip install requests selenium webdriver-manager markdownify\n",
    "\n",
    "requests: Allows sending HTTP requests to the LLaMA model.\n",
    "selenium: Automates browser interactions, which is especially useful for websites with dynamic content.\n",
    "webdriver-manager: Helps manage the correct version of ChromeDriver needed for Selenium.\n",
    "markdownify: Converts HTML content into Markdown format for easier processing by LLaMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39568531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting markdownify\n",
      "  Downloading markdownify-1.1.0-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting typing_extensions~=4.9 (from selenium)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging in /Users/ulrike_imac_air/projects/code/code_env/lib/python3.13/site-packages (from webdriver-manager) (25.0)\n",
      "Collecting beautifulsoup4<5,>=4.9 (from markdownify)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: six<2,>=1.15 in /Users/ulrike_imac_air/projects/code/code_env/lib/python3.13/site-packages (from markdownify) (1.17.0)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5,>=4.9->markdownify)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading markdownify-1.1.0-py3-none-any.whl (13 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl (199 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: sortedcontainers, websocket-client, urllib3, typing_extensions, soupsieve, sniffio, python-dotenv, pysocks, idna, h11, charset-normalizer, certifi, attrs, wsproto, requests, outcome, beautifulsoup4, webdriver-manager, trio, markdownify, trio-websocket, selenium\n",
      "Successfully installed attrs-25.3.0 beautifulsoup4-4.13.4 certifi-2025.4.26 charset-normalizer-3.4.2 h11-0.16.0 idna-3.10 markdownify-1.1.0 outcome-1.3.0.post0 pysocks-1.7.1 python-dotenv-1.1.0 requests-2.32.3 selenium-4.32.0 sniffio-1.3.1 sortedcontainers-2.4.0 soupsieve-2.7 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 urllib3-2.4.0 webdriver-manager-4.0.2 websocket-client-1.8.0 wsproto-1.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests selenium webdriver-manager markdownify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a11bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/ulrike_imac_air/projects/code/code_env/lib/python3.13/site-packages (25.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.0.1\n",
      "    Uninstalling pip-25.0.1:\n",
      "      Successfully uninstalled pip-25.0.1\n",
      "Successfully installed pip-25.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2236ef3d",
   "metadata": {},
   "source": [
    "Step 2: Set Up the Selenium WebDriver\n",
    "Next, youâ€™ll need to set up a headless browser using Selenium. This allows you to interact with websites programmatically without opening a visual browser.\n",
    "\n",
    "Hereâ€™s how to initialize a headless browser with Chrome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa80071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "options = Options()\n",
    "options.add_argument(\" - headless\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd22b7",
   "metadata": {},
   "source": [
    "Step 3: Extract HTML from Amazon Product Page\n",
    "Now, youâ€™ll need to extract the HTML content of a product page. Amazon product pages contain the product information within a container, which you can access using Selenium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24cc0c1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n0   chromedriver                        0x0000000100f36a54 cxxbridge1$str$ptr + 2803960\n1   chromedriver                        0x0000000100f2ecf0 cxxbridge1$str$ptr + 2771860\n2   chromedriver                        0x0000000100a7a864 cxxbridge1$string$len + 93028\n3   chromedriver                        0x0000000100ac1410 cxxbridge1$string$len + 382736\n4   chromedriver                        0x0000000100b02480 cxxbridge1$string$len + 649088\n5   chromedriver                        0x0000000100ab57ec cxxbridge1$string$len + 334572\n6   chromedriver                        0x0000000100efbccc cxxbridge1$str$ptr + 2562928\n7   chromedriver                        0x0000000100efef98 cxxbridge1$str$ptr + 2575932\n8   chromedriver                        0x0000000100edc2c4 cxxbridge1$str$ptr + 2433384\n9   chromedriver                        0x0000000100eff810 cxxbridge1$str$ptr + 2578100\n10  chromedriver                        0x0000000100ecd2f0 cxxbridge1$str$ptr + 2371988\n11  chromedriver                        0x0000000100f1f57c cxxbridge1$str$ptr + 2708512\n12  chromedriver                        0x0000000100f1f708 cxxbridge1$str$ptr + 2708908\n13  chromedriver                        0x0000000100f2e93c cxxbridge1$str$ptr + 2770912\n14  libsystem_pthread.dylib             0x0000000186931c0c _pthread_start + 136\n15  libsystem_pthread.dylib             0x000000018692cb80 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mselenium\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwebdriver\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msupport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m expected_conditions \u001b[38;5;28;01mas\u001b[39;00m EC\n\u001b[32m      4\u001b[39m wait = WebDriverWait(driver, \u001b[32m15\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m product_container = \u001b[43mwait\u001b[49m\u001b[43m.\u001b[49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mppd\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m page_html = product_container.get_attribute(\u001b[33m\"\u001b[39m\u001b[33mouterHTML\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/code/code_env/lib/python3.13/site-packages/selenium/webdriver/support/wait.py:146\u001b[39m, in \u001b[36mWebDriverWait.until\u001b[39m\u001b[34m(self, method, message)\u001b[39m\n\u001b[32m    144\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    145\u001b[39m     time.sleep(\u001b[38;5;28mself\u001b[39m._poll)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[31mTimeoutException\u001b[39m: Message: \nStacktrace:\n0   chromedriver                        0x0000000100f36a54 cxxbridge1$str$ptr + 2803960\n1   chromedriver                        0x0000000100f2ecf0 cxxbridge1$str$ptr + 2771860\n2   chromedriver                        0x0000000100a7a864 cxxbridge1$string$len + 93028\n3   chromedriver                        0x0000000100ac1410 cxxbridge1$string$len + 382736\n4   chromedriver                        0x0000000100b02480 cxxbridge1$string$len + 649088\n5   chromedriver                        0x0000000100ab57ec cxxbridge1$string$len + 334572\n6   chromedriver                        0x0000000100efbccc cxxbridge1$str$ptr + 2562928\n7   chromedriver                        0x0000000100efef98 cxxbridge1$str$ptr + 2575932\n8   chromedriver                        0x0000000100edc2c4 cxxbridge1$str$ptr + 2433384\n9   chromedriver                        0x0000000100eff810 cxxbridge1$str$ptr + 2578100\n10  chromedriver                        0x0000000100ecd2f0 cxxbridge1$str$ptr + 2371988\n11  chromedriver                        0x0000000100f1f57c cxxbridge1$str$ptr + 2708512\n12  chromedriver                        0x0000000100f1f708 cxxbridge1$str$ptr + 2708908\n13  chromedriver                        0x0000000100f2e93c cxxbridge1$str$ptr + 2770912\n14  libsystem_pthread.dylib             0x0000000186931c0c _pthread_start + 136\n15  libsystem_pthread.dylib             0x000000018692cb80 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "wait = WebDriverWait(driver, 15)\n",
    "product_container = wait.until(EC.presence_of_element_located((By.ID, \"ppd\")))\n",
    "page_html = product_container.get_attribute(\"outerHTML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21690314",
   "metadata": {},
   "source": [
    "Step 4: Convert HTML to Markdown\n",
    "Convert the extracted HTML into Markdown to optimize LLaMAâ€™s processing. Markdown is a cleaner and more efficient format for processing large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a18cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdownify import markdownify as md\n",
    "clean_text = md(page_html, heading_style=\"ATX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ce0bd",
   "metadata": {},
   "source": [
    "Step 5: Create a Structured Data Extraction Prompt\n",
    "The key to successful scraping with LLaMA 3 is crafting the right prompt. The prompt instructs LLaMA on what data to extract from the provided content.\n",
    "\n",
    "Hereâ€™s a sample prompt for extracting product details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe0adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = (\n",
    "\"You are an expert Amazon product data extractor. Your task is to extract product data from the provided content. \"\n",
    "\"Return ONLY valid JSON with EXACTLY the following fields and formats:\\n\\n\"\n",
    "\"{\\n\"\n",
    "' \"title\": \"string - the product title\",\\n'\n",
    "' \"price\": number - the current price (numerical value only)\",\\n'\n",
    "' \"original_price\": number or null - the original price if available,\\n'\n",
    "' \"discount\": number or null - the discount percentage if available,\\n'\n",
    "' \"rating\": number or null - the average rating (0â€“5 scale),\\n'\n",
    "' \"review_count\": number or null - total number of reviews,\\n'\n",
    "' \"description\": \"string - main product description\",\\n'\n",
    "' \"features\": [\"string\"] - list of bullet point features,\\n'\n",
    "' \"availability\": \"string - stock status\",\\n'\n",
    "' \"asin\": \"string - 10-character Amazon ID\"\\n'\n",
    "\"}\\n\\n\"\n",
    "\"Return ONLY the JSON without any additional text.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da105277",
   "metadata": {},
   "source": [
    "Step 6: Call the LLaMA API\n",
    "With the Markdown content ready, send it to the LLaMA API to extract the structured data. Youâ€™ll use the following Python code to send the request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0189e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "response = requests.post(\n",
    "\"http://localhost:11434/api/generate\",\n",
    "json={\n",
    "\"model\": \"llama3.1:8b\",\n",
    "\"prompt\": f\"{PROMPT}\\n\\n{clean_text}\",\n",
    "\"stream\": False,\n",
    "\"format\": \"json\",\n",
    "\"options\": {\n",
    "\"temperature\": 0.1,\n",
    "\"num_ctx\": 12000,\n",
    "},\n",
    "},\n",
    "timeout=250,\n",
    ")\n",
    "raw_output = response.json()[\"response\"].strip()\n",
    "product_data = json.loads(raw_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d678d76",
   "metadata": {},
   "source": [
    "Step 7: Save the Results\n",
    "Finally, save the extracted product data to a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc56109",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"product_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "json.dump(product_data, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
