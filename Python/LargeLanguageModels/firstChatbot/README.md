# **Building and Deploying Your First LLM Chatbot is Easier Than You Think**
**Just Follow these 5 Steps (Using Free Tools Only)**

**by Anjolaoluwa Ajayi**

---

## **Introduction — Step 0**
When you want to build your first LLM chatbot, the first step is to identify the main purpose and functionalities of the chatbot.

The chatbot we’re building in this tutorial is called **Pep**, short for **Pep Talker**.

**Its purpose/main function:**
- Be a productivity coach for Gen Zs and teenagers.

**Other features:**
- Intuitive chat interface (similar to the user’s everyday messaging app feel).
- Contextual memory (remembers details from earlier messages to keep the conversation flowing naturally).
- Streaming effect (like how ChatGPT generates responses one word at a time).

---

## **Setup Development Environment — Step 1**
If you’re not new to Python programming, you can skip this step and go straight to step 2.

**This step involves:**
- Download and Install **Python** (confirm with `python --version`)
- Download and Install **Anaconda** (for managing libraries and dependencies)
- Download and Install **Git** (confirm with `git --version`)
- Download and Install an **IDE** (we’ll use **VS Code**)
- Install the **Python extension** in VS Code
- Set the **Python interpreter** to your Anaconda environment

If everything works properly, you’re ready to move on to the next step!

---

## **Initialize Project Workspace — Step 2**

**Let’s set up your workspace!**
1. **Create a New Folder** for your project.
2. **Open the Folder in VS Code**.
3. **Create a `.streamlit` Folder** inside your main folder.
4. **Add a `secrets.toml` File** inside `.streamlit` and paste your GROQ API key:
   ```toml
   GROQ_API_KEY = 'gsk_therestofyourapikeygoeshere'
   ```
5. **Add a `.gitignore` File** in your main project folder. Add this line:
   ```
   /.streamlit/secrets.toml
   ```
6. **Create a `requirements.txt` File** and add:
   ```
   streamlit
groq
   ```
7. **Install the required libraries:**
   ```sh
   pip install -r requirements.txt
   ```
8. **Create a `README.md` file** (optional but recommended).
9. **Create Your Main Python File:** Name it `streamlit_app.py`.

Once your project workspace is set, you’re ready to start coding!

---

## **Build the LLM Wrapper — Step 3**

An **LLM Wrapper** is a function or module that takes in an input, passes it to an LLM, and returns the output generated by the LLM.

**First, import required libraries:**
```python
import time
from groq import Groq
import streamlit as st
```

**Add your GROQ API key:**
```python
GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
```

**Initialize a Groq client:**
```python
groq_client = Groq(api_key=GROQ_API_KEY)
```

**Define the system prompt:**
```python
system_message = (
    "You are a highly relatable personal productivity coach for teenagers."
    "Your name is Pep (short for Pep Talker)."
    "Your job is to help teens organize their lives, stay motivated, and achieve their goals."
    "Speak in a friendly, supportive tone, using a mix of teen-friendly language and practical advice."
    "Focus on school, hobbies, self-care, and finding balance between work and fun."
    "Be motivational, empathetic, and slightly witty but always positive."
    "Avoid being overly formal; keep your responses very short, fun, actionable, and encouraging."
    "REMEMBER: Always keep your responses short and concise - Less than 100 words."
)

system_prompt = {
    "role": "system",
    "content": system_message
}
```

**Build the LLM Wrapper:**
```python
def get_response(chat_history):
    response = groq_client.chat.completions.create(
        model="llama3-70b-8192",
        messages=chat_history,
        max_tokens=200,
        temperature=1.2
    )
    chat_response = response.choices[0].message.content
    for word in chat_response.split():
        yield word + " "
        time.sleep(0.05)
```

**Notes:**
- The chat history is a list of messages (system, user, assistant).
- The model used is **Llama 3–70b-8192**.
- `max_tokens=200` keeps responses short.
- `temperature=1.2` makes output more creative.
- The generator yields words one at a time for a streaming effect.

---

## **Create an Interface for the Chatbot — Step 4**

We’ll use **Streamlit** for the chatbot’s interface.

```python
def main():
    st.title("PepTalk")
    with st.expander("About Pep"):
        st.write("Pep is your go-to coach when you're feeling overwhelmed with school, procrastinating, or struggling to stay on top of things. Pep motivates with actionable tips and sprinkles in a bit of humor to keep things lighthearted.")
    if "messages" not in st.session_state:
        st.session_state.messages = [system_prompt]
    for message in st.session_state.messages:
        if message != system_prompt:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
    if prompt := st.chat_input("Tell Pep what's up"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        response = get_response(st.session_state.messages)
        with st.chat_message("assistant"):
            chat_response = st.write_stream(response)
        st.session_state.messages.append({"role": "assistant", "content": chat_response})

if __name__ == "__main__":
    main()
```

**How contextual memory works:**
- `st.session_state` keeps track of messages between the user and Pep.
- The list `st.session_state.messages` is passed to the LLM wrapper as chat history.

---

## **Run the App**

Open your terminal and run:
```sh
streamlit run streamlit_app.py
```

For better performance, install the Watchdog module:
```sh
xcode-select --install
pip install watchdog
```

---

## **Deploy the Chatbot — Step 5**

### **Push your Code to GitHub**
1. Create a GitHub account (if you don’t have one).
2. Create a new repository.
3. In VS Code, connect to GitHub:
   ```sh
   git config --global user.name "Your GitHub username"
   git config --global user.email "your-email@example.com"
   ```
4. Clone your repository:
   ```sh
   git clone https://github.com/your-username/your-repository.git
   ```
5. Initialize git (if needed):
   ```sh
   git init
   ```
6. Link your local project folder to the remote repo:
   ```sh
   git remote add origin https://github.com/your-username/your-repository.git
   ```
7. Add, commit, and push your files:
   ```sh
   git add .
   git commit -m "Initial commit"
   git push -u origin main
   ```

### **Deploy with Streamlit Share**
1. Create a Streamlit Share account (use GitHub for authentication).
2. Click **Create New App**.
3. Select your repo and set the main file path as `streamlit_app.py`.
4. In advanced settings, add your API key in the secrets area as in your `secrets.toml` file.
5. Click **Deploy**.

In 2–3 minutes, your chatbot should be live!

---

**That’s it! You did it!**

---

*For questions or clarifications, feel free to ask in the comments.*


